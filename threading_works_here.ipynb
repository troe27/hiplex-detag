{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from pathos.helpers import mp as multiprocess\n",
    "from pathos import multiprocessing as mp\n",
    "import argparse\n",
    "from Bio.Seq import Seq\n",
    "from Bio.Alphabet import generic_dna\n",
    "from array import array\n",
    "from Bio import SeqIO\n",
    "import gzip\n",
    "from collections import Counter\n",
    "from detag_v4 import *\n",
    "from detag_v4 import get_tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def make_job_dict_manually(heel,\n",
    "                           gsp,\n",
    "                           tags,\n",
    "                           h3score,\n",
    "                           h5score,\n",
    "                           outputprefix,\n",
    "                           pairedfastq,\n",
    "                           forward,\n",
    "                           reverse,\n",
    "                           filtering,\n",
    "                           pairedparams=True,\n",
    "                           unpairedfastq=None,\n",
    "                          ):\n",
    "\n",
    "    \n",
    "    job_summary = [\"starting Job with the following parameters:\"]\n",
    "    job = dict()\n",
    "    job[\"heel_file\"] = heel\n",
    "    job[\"gsp\"] = gsp\n",
    "    job_summary.append(\"heel file is: \"+str(job['heel_file']))\n",
    "    job[\"tag_file\"] = tags\n",
    "    job_summary.append(\"tag file is: \"+str(job['tag_file']))\n",
    "    if h3score and h5score is None:\n",
    "        job_summary.append(\"using default values for primerscores\")\n",
    "        job[\"h3score\"] = float(0.9)\n",
    "        job[\"h5score\"] = float(0.9)\n",
    "    else:\n",
    "        job[\"h3score\"] = float(h3score)\n",
    "        job[\"h5score\"] = float(h5score)\n",
    "\n",
    "    job[\"output_prefix\"] = outputprefix\n",
    "\n",
    "    job_summary.append(\"primerscore (match/length ratio)  3'>{}  5'>{}\".format(job[\"h3score\"], job[\"h5score\"]))\n",
    "    if pairedfastq is True:\n",
    "        job[\"filetype\"] = \"pairedfastq\"\n",
    "        job[\"forward_file\"] = forward\n",
    "        job[\"reverse_file\"] = reverse\n",
    "    if pairedfastq is not True:\n",
    "        job[\"filetype\"] = \"unpairedfastq\"\n",
    "    job[\"unpaired_file\"] = unpairedfastq\n",
    "    if (unpairedfastq and forward) or (unpairedfastq and reverse ) is not None:\n",
    "        print \" if the reads are not paired, you should not specify --forward and --reverse\"\n",
    "        exit()\n",
    "\n",
    "    job_summary.append(\"output_prefix is {}\".format(job[\"output_prefix\"]))\n",
    "    job_summary.append(\"file type is : {} \".format(job[\"filetype\"]))\n",
    "    job_summary.append(\"input files are : {} , {}, {}\".format(job['forward_file'],\n",
    "                                                              job['reverse_file'],\n",
    "                                                              job['unpaired_file']))\n",
    "\n",
    "    if pairedparams is not None and pairedfastq is True:\n",
    "        param_list = pairedparams.split(\":\")\n",
    "        job[\"kmer_overlap\"] = int(param_list[0])\n",
    "        job[\"hsp_overlap\"] = int(param_list[1])\n",
    "        job[\"min_overlap\"] = int(param_list[2])\n",
    "    elif pairedparams is None and pairedfastq is True:\n",
    "        job_summary.append(\"no pairedparams found, using default parameters.\")\n",
    "        job[\"kmer_overlap\"] = 7\n",
    "        job[\"hsp_overlap\"] = 5\n",
    "        job[\"min_overlap\"] = 10\n",
    "\n",
    "    elif pairedparams is not None and pairedfastq is not True:\n",
    "        job_summary.append(\"paired - parameters only relevant for paired fastq files, ignoring parameters.\")\n",
    "    if pairedfastq is True:\n",
    "        job_summary.append('using paired-parameters kmer_overlap = {} , hsp_overlap = {} , min_overlap = {}'.format(job[\"kmer_overlap\"],\n",
    "                                                                                                                    job[\"hsp_overlap\"],\n",
    "                                                                                                                    job[\"min_overlap\"]))\n",
    "    filter_list = filtering.split(\":\")\n",
    "    job[\"filter_strategy\"] = int(filter_list[0])\n",
    "    job[\"filter_minlen\"] = int(filter_list[1])\n",
    "    job[\"filter_maxlen\"] = int(filter_list[2])\n",
    "    job[\"filter_meanqual\"] = int(filter_list[3])\n",
    "    job[\"filter_minqual\"] = int(filter_list[4])\n",
    "\n",
    "    job_summary.append(\"filtering strategy {} , with minlen {} , maxlen {}, meanqual {} , minqual {}\".format(job[\"filter_strategy\"],\n",
    "                                                                                                             job[\"filter_minlen\"],\n",
    "                                                                                                             job[\"filter_maxlen\"],\n",
    "                                                                                                             job[\"filter_meanqual\"],\n",
    "                                                                                                             job[\"filter_minqual\"]))\n",
    "    return job, job_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "job,jobsum = make_job_dict_manually(filtering=\"1:50:150:20:15\",forward=\"test_data/first_10k_R1.fastq\", reverse=\"test_data/first_10k_R2.fastq\", gsp=\"test_data/tabulated_relevant_primers.txt\", h3score=\"0.9\", h5score=\"0.9\", heel=\"test_data/heel.txt\", outputprefix=\"test_data/debug_15_8_\", pairedfastq=True, pairedparams=\"7:5:10\", tags=\"test_data/tags_no_CAA.txt\",) #FIXME add input here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_data = Pair(fastq_1=job[\"forward_file\"],\n",
    "                fastq_2=job[\"reverse_file\"],\n",
    "                hsp=job[\"hsp_overlap\"],\n",
    "                kmer=job[\"kmer_overlap\"],\n",
    "                min=job[\"min_overlap\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a =(qseq for qseq in seq_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "u = list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "r = a.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l = r.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def new_main():\n",
    "    job,jobsum = make_job_dict_manually(filtering=\"1:50:150:20:15\",forward=\"test_data/first_10k_R1.fastq\", reverse=\"test_data/first_10k_R2.fastq\", gsp=\"test_data/tabulated_relevant_primers.txt\", h3score=\"0.9\", h5score=\"0.9\", heel=\"test_data/heel.txt\", outputprefix=\"test_data/debug_15_8_\", pairedfastq=True, pairedparams=\"7:5:10\", tags=\"test_data/tags_no_CAA.txt\") #FIXME add input here\n",
    "    tags5,tagsum = get_tagset(tag_input_file=job[\"tag_file\"],end='3')\n",
    "    tags3 = get_tagset(tag_input_file=job[\"tag_file\"],end='5')[0]\n",
    "    if job[\"gsp\"]:\n",
    "        gsp3,gspsum = get_tagset(tag_input_file=job[\"gsp\"],end='3')\n",
    "        gsp5 = get_tagset(tag_input_file=job[\"gsp\"],end='5')[0]\n",
    "\n",
    "    heel5, heel3, heelsum = get_heels(job[\"heel_file\"])\n",
    "    seq_data = Pair(fastq_1=job[\"forward_file\"],\n",
    "                    fastq_2=job[\"reverse_file\"],\n",
    "                    hsp=job[\"hsp_overlap\"],\n",
    "                    kmer=job[\"kmer_overlap\"],\n",
    "                    min=job[\"min_overlap\"])\n",
    "    # init DeTagSeq\n",
    "    if not job[\"gsp\"]:\n",
    "        dtseq = DeTagSeq(p3=heel3, p3s=job[\"h3score\"], p5=heel5, p5s=job[\"h5score\"], t3=tags3, t5=tags5)\n",
    "    else:\n",
    "        dtseq = DeTagSeq(p3=heel3, p3s=job[\"h3score\"], p5=heel5, p5s=job[\"h5score\"], t3=tags3, t5=tags5, gsp3=gsp3, gsp5=gsp5)\n",
    "\n",
    "    # init ResultProcessor\n",
    "    if not job[\"gsp\"]:\n",
    "        reproc = ResultProcessor(tags=tags3, output_prefix=job[\"output_prefix\"])\n",
    "    else:\n",
    "        reproc = ResultProcessor(tags=tags3, output_prefix=job[\"output_prefix\"], gsp=gsp3)\n",
    "    # init threading:\n",
    "    qseq_gen = (qseq for qseq in seq_data)\n",
    "    td = ThreadingController(work=list(qseq_gen))\n",
    "    # execute detag & reproc\n",
    "    reproc.open_tag_files()\n",
    "    # create manager, queue\n",
    "    man, q = td.create_q()\n",
    "    # execute controller\n",
    "    td.controller(q)\n",
    "    \n",
    "    reproc.close_tag_files()\n",
    "    reproc.harvest_stats()\n",
    "    reproc.write_stats()\n",
    "    write_logfile(job[\"output_prefix\"],jobsum, heelsum, tagsum,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ThreadingController(object):\n",
    "    ''' base class for multithreading jobs'''\n",
    "    def __init__(self, max_proc=1, work=None):\n",
    "        self.max_proc = max_proc\n",
    "        self.work = work\n",
    "\n",
    "    def create_q(self):\n",
    "        '''creates the manager and the queue as external objects.\n",
    "            multiprocess/pickling dont play well with Classes '''\n",
    "        manager = multiprocess.Manager()\n",
    "        queue = manager.Queue()\n",
    "        return manager, queue\n",
    "\n",
    "    def worker(self, work):\n",
    "        '''runs analysis on seq_data'''\n",
    "        r=work.get()\n",
    "        res = dict(status = dict())\n",
    "        seq = r[0]\n",
    "        qual = r[1]\n",
    "        detagged_seq = None\n",
    "        if seq:\n",
    "            res[\"id\"] = seq.id\n",
    "            if job[\"filter_strategy\"] == 0: # Filter full ###\n",
    "                if not qual:\n",
    "                    raise Exception(\"Full sequence filtering requires quality data\")\n",
    "\n",
    "                seq, status = filter_full(seq_record=seq,\n",
    "                                          qual=qual,\n",
    "                                          min_length=job[\"filter_minlen\"],\n",
    "                                          mean_min=job[\"filter_meanqual\"],\n",
    "                                          min_qual=job[\"filter_minqual\"])\n",
    "            elif job[\"filter_strategy\"] == 1: # Full sequence, no filtering###\n",
    "                status = None\n",
    "            elif job[\"filter_strategy\"] == 2: # HQR###\n",
    "                if not qual:\n",
    "                    raise Exception(\"HQR filtering requires quality data\")\n",
    "                seq, status = filter_hqr(seq_record=seq,\n",
    "                                         qual=qual,\n",
    "                                         min_length=job[\"filter_minlen\"],\n",
    "                                         mean_min=job[\"filter_meanqual\"],\n",
    "                                         min_qual=job[\"filter_minqual\"])\n",
    "            elif job[\"filter_strategy\"] == 3: # Primers first\n",
    "                if not qual:\n",
    "                    raise Exception(\"Amplicon quality filtering requires quality data\")\n",
    "                detagged_seq, qual = dtseq.detag_seq(seq,qual)\n",
    "                if \"status\" in detagged_seq:\n",
    "                    res[\"status\"][detagged_seq[\"status\"]] = 1\n",
    "                    res[\"detagged_seq\"] = detagged_seq\n",
    "                    #result_list.append(res)\n",
    "                    return res ###\n",
    "                    \n",
    "\n",
    "                seq, status = filter_full(seq_record=detagged_seq[\"seq_record\"],\n",
    "                                          qual=qual,\n",
    "                                          min_length=job[\"filter_minlen\"],\n",
    "                                          mean_min=job[\"filter_meanqual\"],\n",
    "                                          min_qual=job[\"filter_minqual\"])\n",
    "            else:\n",
    "                raise Exception(\"Unknown filter type\")\n",
    "        else:\n",
    "            status = \"failed_pair\"\n",
    "        if status:\n",
    "            res[\"status\"][status] = 1\n",
    "            #result_list.append(res)\n",
    "            return res ###\n",
    "            \n",
    "        if len(seq.seq) < job[\"filter_minlen\"]:\n",
    "            res[\"status\"][\"too_short\"] = 1\n",
    "            #result_list.append(res)\n",
    "            return res ###\n",
    "           \n",
    "\n",
    "        if not detagged_seq:\n",
    "            detagged_seq, qual = dtseq.detag_seq(seq, qual)\n",
    "            res[\"detagged_seq\"] = detagged_seq\n",
    "        if \"status\" in detagged_seq:\n",
    "            res[\"status\"][detagged_seq[\"status\"]] = 1\n",
    "                #result_list.append(res)\n",
    "            res[\"detagged_seq\"] = detagged_seq\n",
    "            return res ###\n",
    "           \n",
    "        if len(detagged_seq[\"seq\"]) < job[\"filter_minlen\"]:\n",
    "            res[\"status\"][\"too_short\"] = 1\n",
    "            res[\"detagged_seq\"] = detagged_seq\n",
    "                #result_list.append(res)\n",
    "            return res\n",
    "\n",
    "\n",
    "    def watcher(self,q):\n",
    "        ''' watches the queue until the kill-word is on the queue\n",
    "            processes items on the queue via self.handling()'''\n",
    "        while True:\n",
    "            a = q.get()\n",
    "            if a == \"kill\":\n",
    "                break\n",
    "            else:\n",
    "                self.handling(a)\n",
    "            \n",
    "\n",
    "    def handling(self, result):\n",
    "        reproc.process_result(result)\n",
    "        return None\n",
    "\n",
    "    def controller(self, q):\n",
    "        ''' controls the worker and watcher processes\n",
    "            passes the kill word to the queue once all jobs are done'''\n",
    "        # create ProcessPool\n",
    "        pool = mp.ProcessPool(processes=self.max_proc)\n",
    "        # start the watcher\n",
    "        eye = pool.uimap(self.watcher, (q,))\n",
    "        #print len(self.work)\n",
    "        #print type(self.work)\n",
    "        # map workers to the input, put the results into the writing queue\n",
    "        for i in pool.uimap(self.worker, self.work):\n",
    "            q.put(i)\n",
    "        # terminate watcher\n",
    "        q.put(\"kill\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def worker(work):\n",
    "    '''runs analysis on seq_data'''\n",
    "    r=work.get()\n",
    "    res = dict(status = dict())\n",
    "    seq = r[0]\n",
    "    qual = r[1]\n",
    "    detagged_seq = None\n",
    "    if seq:\n",
    "        res[\"id\"] = seq.id\n",
    "        if job[\"filter_strategy\"] == 0: # Filter full ###\n",
    "            if not qual:\n",
    "                raise Exception(\"Full sequence filtering requires quality data\")\n",
    "\n",
    "            seq, status = filter_full(seq_record=seq,\n",
    "                                        qual=qual,\n",
    "                                        min_length=job[\"filter_minlen\"],\n",
    "                                        mean_min=job[\"filter_meanqual\"],\n",
    "                                        min_qual=job[\"filter_minqual\"])\n",
    "        elif job[\"filter_strategy\"] == 1: # Full sequence, no filtering###\n",
    "            status = None\n",
    "        elif job[\"filter_strategy\"] == 2: # HQR###\n",
    "            if not qual:\n",
    "                raise Exception(\"HQR filtering requires quality data\")\n",
    "            seq, status = filter_hqr(seq_record=seq,\n",
    "                                     qual=qual,\n",
    "                                     min_length=job[\"filter_minlen\"],\n",
    "                                     mean_min=job[\"filter_meanqual\"],\n",
    "                                     min_qual=job[\"filter_minqual\"])\n",
    "        elif job[\"filter_strategy\"] == 3: # Primers first\n",
    "            if not qual:\n",
    "                raise Exception(\"Amplicon quality filtering requires quality data\")\n",
    "            detagged_seq, qual = dtseq.detag_seq(seq,qual)\n",
    "            if \"status\" in detagged_seq:\n",
    "                res[\"status\"][detagged_seq[\"status\"]] = 1\n",
    "                res[\"detagged_seq\"] = detagged_seq\n",
    "                #result_list.append(res)\n",
    "                return res ###\n",
    "                    \n",
    "\n",
    "            seq, status = filter_full(seq_record=detagged_seq[\"seq_record\"],\n",
    "                                      qual=qual,\n",
    "                                      min_length=job[\"filter_minlen\"],\n",
    "                                      mean_min=job[\"filter_meanqual\"],\n",
    "                                      min_qual=job[\"filter_minqual\"])\n",
    "        else:\n",
    "            raise Exception(\"Unknown filter type\")\n",
    "    else:\n",
    "        status = \"failed_pair\"\n",
    "    if status:\n",
    "        res[\"status\"][status] = 1\n",
    "        #result_list.append(res)\n",
    "        return res ###\n",
    "            \n",
    "    if len(seq.seq) < job[\"filter_minlen\"]:\n",
    "        res[\"status\"][\"too_short\"] = 1\n",
    "        #result_list.append(res)\n",
    "        return res ###\n",
    "           \n",
    "\n",
    "    if not detagged_seq:\n",
    "        detagged_seq, qual = dtseq.detag_seq(seq, qual)\n",
    "        res[\"detagged_seq\"] = detagged_seq\n",
    "        \n",
    "    if \"status\" in detagged_seq:\n",
    "        res[\"status\"][detagged_seq[\"status\"]] = 1\n",
    "            #result_list.append(res)\n",
    "        res[\"detagged_seq\"] = detagged_seq\n",
    "        return res ###\n",
    "           \n",
    "    if len(detagged_seq[\"seq\"]) < job[\"filter_minlen\"]:\n",
    "        res[\"status\"][\"too_short\"] = 1\n",
    "        res[\"detagged_seq\"] = detagged_seq\n",
    "            #result_list.append(res)\n",
    "        return res\n",
    "\n",
    "    if job[\"filter_maxlen\"] > 0 and len(detagged_seq[\"seq\"]) > job[\"filter_maxlen\"]:\n",
    "        res[\"status\"][\"truncated\"] = 1\n",
    "        detagged_seq[\"seq\"] = detagged_seq[\"seq\"][:job[\"filter_maxlen\"]]\n",
    "        res[\"detagged_seq\"] = detagged_seq\n",
    "        # return res\n",
    "\n",
    "    if job[\"filter_maxlen\"] < 0:\n",
    "        res[\"status\"][\"truncated\"] = 1\n",
    "        detagged_seq[\"seq\"] = detagged_seq[\"seq\"][:job[\"filter_maxlen\"]]\n",
    "        res[\"detagged_seq\"] = detagged_seq\n",
    "#       # return res\n",
    "\n",
    "#        print res\n",
    "    res[\"keep\"] = True\n",
    "    res[\"id\"] = seq.id\n",
    "    res[\"detag_qual\"] = qual\n",
    "        #reproc.process_result(res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def watcher(q, job):\n",
    "    ''' watches the queue until the kill-word is on the queue\n",
    "        processes items on the queue via self.handling()'''\n",
    "    reproc.open_tag_files()   \n",
    "    while True:\n",
    "        a = q.get()\n",
    "        if a == \"kill\":\n",
    "            reproc.close_tag_files()\n",
    "            reproc.harvest_stats()\n",
    "            reproc.write_stats()\n",
    "            break\n",
    "        else:\n",
    "            #dummy.write(\"dummy \\n\")\n",
    "            if a is not None:\n",
    "                reproc.process_result(a)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "job,jobsum = make_job_dict_manually(tags=\"test_data/tags_no_CAA.txt\",\n",
    "                                    heel=\"test_data/heel.txt\",\n",
    "                                    pairedfastq=True,\n",
    "                                    forward=\"test_data/first_100k_R1.fastq\",\n",
    "                                    reverse=\"test_data/first_100k_R2.fastq\",\n",
    "                                    pairedparams=\"7:5:10\",\n",
    "                                    outputprefix=\"test_data/debuggy_15_8_\",\n",
    "                                    h3score=\"0.9\",\n",
    "                                    h5score=\"0.9\",\n",
    "                                    filtering=\"1:50:150:20:15\",\n",
    "                                    gsp=\"test_data/tabulated_relevant_primers.txt\")\n",
    "                                     #FIXME add input here\n",
    "tags5,tagsum = get_tagset(tag_input_file=job[\"tag_file\"],end='3')\n",
    "tags3 = get_tagset(tag_input_file=job[\"tag_file\"],end='5')[0]\n",
    "if job[\"gsp\"]:\n",
    "    gsp3,gspsum = get_tagset(tag_input_file=job[\"gsp\"],end='3')\n",
    "    gsp5 = get_tagset(tag_input_file=job[\"gsp\"],end='5')[0]\n",
    "\n",
    "heel5, heel3, heelsum = get_heels(job[\"heel_file\"])\n",
    "seq_data = Pair(fastq_1=job[\"forward_file\"],\n",
    "                fastq_2=job[\"reverse_file\"],\n",
    "                hsp=job[\"hsp_overlap\"],\n",
    "                kmer=job[\"kmer_overlap\"],\n",
    "                min=job[\"min_overlap\"])\n",
    "# init DeTagSeq\n",
    "if not job[\"gsp\"]:\n",
    "    dtseq = DeTagSeq(p3=heel3, p3s=job[\"h3score\"], p5=heel5, p5s=job[\"h5score\"], t3=tags3, t5=tags5)\n",
    "else:\n",
    "    dtseq = DeTagSeq(p3=heel3, p3s=job[\"h3score\"], p5=heel5, p5s=job[\"h5score\"], t3=tags3, t5=tags5, gsp3=gsp3, gsp5=gsp5)\n",
    "# init ResultProcessor\n",
    "if not job[\"gsp\"]:\n",
    "    reproc = ResultProcessor(tags=tags3, output_prefix=job[\"output_prefix\"])\n",
    "else:\n",
    "    reproc = ResultProcessor(tags=tags3, output_prefix=job[\"output_prefix\"], gsp=gsp3)\n",
    "# init threading:\n",
    "pool = mp.ProcessPool()\n",
    "qseq_gen = (qseq for qseq in seq_data)\n",
    "\n",
    "\n",
    "# create manager, queue\n",
    "man = multiprocess.Manager()\n",
    "q = man.Queue()\n",
    "# execute controller\n",
    "#dummy = open(\"dummy.txt\", \"a\")\n",
    "eye = pool.uimap(watcher, (q,),job )    \n",
    "for i in pool.uimap(worker, list(qseq_gen)):\n",
    "    q.put(i)\n",
    "#pool.uimap(put_worker, list(qseq_gen))\n",
    "#dummy.close()\n",
    "    \n",
    "    \n",
    "q.put(\"kill\")\n",
    "#write_logfile(job[\"output_prefix\"],jobsum, heelsum, tagsum,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "watcher(q, job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.qsize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'status': {'no_primer3': 1}, 'id': 'M00485:302:000000000-AM3NR:1:1101:15825:1343', 'detagged_seq': {'status': 'no_primer3', 'tag_name': 'Hiplex_tag_6', 'gsprim_name': 'Pa_31_1', 'rev': True}}\n"
     ]
    }
   ],
   "source": [
    "a=q.get()\n",
    "print a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    job,jobsum = make_job_dict_manually(tags=\"test_data/tags_no_CAA.txt\",\n",
    "                                    heel=\"test_data/heel.txt\",\n",
    "                                    pairedfastq=True,\n",
    "                                    forward=\"test_data/first_100k_R1.fastq\",\n",
    "                                    reverse=\"test_data/first_100k_R2.fastq\",\n",
    "                                    pairedparams=\"7:5:10\",\n",
    "                                    outputprefix=\"test_data/debby_bug_15_8_\",\n",
    "                                    h3score=\"0.9\",\n",
    "                                    h5score=\"0.9\",\n",
    "                                    filtering=\"1:50:150:20:15\",\n",
    "                                    gsp=\"test_data/tabulated_relevant_primers.txt\")\n",
    "                                     #FIXME add input here\n",
    "    tags5,tagsum = get_tagset(tag_input_file=job[\"tag_file\"],end='3')\n",
    "    tags3 = get_tagset(tag_input_file=job[\"tag_file\"],end='5')[0]\n",
    "    if job[\"gsp\"]:\n",
    "        gsp3,gspsum = get_tagset(tag_input_file=job[\"gsp\"],end='3')\n",
    "        gsp5 = get_tagset(tag_input_file=job[\"gsp\"],end='5')[0]\n",
    "\n",
    "    heel5, heel3, heelsum = get_heels(job[\"heel_file\"])\n",
    "    seq_data = Pair(fastq_1=job[\"forward_file\"],\n",
    "                    fastq_2=job[\"reverse_file\"],\n",
    "                    hsp=job[\"hsp_overlap\"],\n",
    "                    kmer=job[\"kmer_overlap\"],\n",
    "                    min=job[\"min_overlap\"])\n",
    "    # init DeTagSeq\n",
    "    if not job[\"gsp\"]:\n",
    "        dtseq = DeTagSeq(p3=heel3, p3s=job[\"h3score\"], p5=heel5, p5s=job[\"h5score\"], t3=tags3, t5=tags5)\n",
    "    else:\n",
    "        dtseq = DeTagSeq(p3=heel3, p3s=job[\"h3score\"], p5=heel5, p5s=job[\"h5score\"], t3=tags3, t5=tags5, gsp3=gsp3, gsp5=gsp5)\n",
    "\n",
    "    # init ResultProcessor\n",
    "    if not job[\"gsp\"]:\n",
    "        reproc = ResultProcessor(tags=tags3, output_prefix=job[\"output_prefix\"])\n",
    "    else:\n",
    "        reproc = ResultProcessor(tags=tags3, output_prefix=job[\"output_prefix\"], gsp=gsp3)\n",
    "\n",
    "    # execute detag & reproc\n",
    "    reproc.open_tag_files()\n",
    "\n",
    "    for qseq in seq_data:\n",
    "        r=qseq.get()\n",
    "        res = dict(status = dict())\n",
    "        seq = r[0]\n",
    "        qual = r[1]\n",
    "        detagged_seq = None\n",
    "        if seq:\n",
    "            res[\"id\"] = seq.id\n",
    "            if job[\"filter_strategy\"] == 0: # Filter full ###\n",
    "                if not qual:\n",
    "                    raise Exception(\"Full sequence filtering requires quality data\")\n",
    "\n",
    "                seq, status = filter_full(seq_record=seq,\n",
    "                                          qual=qual,\n",
    "                                          min_length=job[\"filter_minlen\"],\n",
    "                                          mean_min=job[\"filter_meanqual\"],\n",
    "                                          min_qual=job[\"filter_minqual\"])\n",
    "            elif job[\"filter_strategy\"] == 1: # Full sequence, no filtering###\n",
    "                status = None\n",
    "            elif job[\"filter_strategy\"] == 2: # HQR###\n",
    "                if not qual:\n",
    "                    raise Exception(\"HQR filtering requires quality data\")\n",
    "                seq, status = filter_hqr(seq_record=seq,\n",
    "                                         qual=qual,\n",
    "                                         min_length=job[\"filter_minlen\"],\n",
    "                                         mean_min=job[\"filter_meanqual\"],\n",
    "                                         min_qual=job[\"filter_minqual\"])\n",
    "            elif job[\"filter_strategy\"] == 3: # Primers first\n",
    "                if not qual:\n",
    "                    raise Exception(\"Amplicon quality filtering requires quality data\")\n",
    "                detagged_seq, qual = dtseq.detag_seq(seq,qual)\n",
    "                if \"status\" in detagged_seq:\n",
    "                    res[\"status\"][detagged_seq[\"status\"]] = 1\n",
    "                    res[\"detagged_seq\"] = detagged_seq\n",
    "                    #result_list.append(res)\n",
    "                    reproc.process_result(res) ###\n",
    "                    continue\n",
    "\n",
    "                seq, status = filter_full(seq_record=detagged_seq[\"seq_record\"],\n",
    "                                          qual=qual,\n",
    "                                          min_length=job[\"filter_minlen\"],\n",
    "                                          mean_min=job[\"filter_meanqual\"],\n",
    "                                          min_qual=job[\"filter_minqual\"])\n",
    "\n",
    "            else:\n",
    "                raise Exception(\"Unknown filter type\")\n",
    "        else:\n",
    "            status = \"failed_pair\"\n",
    "        if status:\n",
    "            res[\"status\"][status] = 1\n",
    "            #result_list.append(res)\n",
    "            reproc.process_result(res) ###\n",
    "            continue\n",
    "        if len(seq.seq) < job[\"filter_minlen\"]:\n",
    "            res[\"status\"][\"too_short\"] = 1\n",
    "            #result_list.append(res)\n",
    "            reproc.process_result(res) ###\n",
    "            continue\n",
    "\n",
    "        if not detagged_seq:\n",
    "            detagged_seq, qual = dtseq.detag_seq(seq, qual)\n",
    "    #print detagged_seq\n",
    "\n",
    "            res[\"detagged_seq\"] = detagged_seq\n",
    "        if \"status\" in detagged_seq:\n",
    "#            print detagged_seq[\"status\"]\n",
    "            res[\"status\"][detagged_seq[\"status\"]] = 1\n",
    "            #result_list.append(res)\n",
    "            res[\"detagged_seq\"] = detagged_seq\n",
    "            reproc.process_result(res) ###\n",
    "            continue\n",
    "        if len(detagged_seq[\"seq\"]) < job[\"filter_minlen\"]:\n",
    "            res[\"status\"][\"too_short\"] = 1\n",
    "            res[\"detagged_seq\"] = detagged_seq\n",
    "            #result_list.append(res)\n",
    "            reproc.process_result(res) ###\n",
    "            continue\n",
    "\n",
    "        if job[\"filter_maxlen\"] > 0 and len(detagged_seq[\"seq\"]) > job[\"filter_maxlen\"]:\n",
    "            res[\"status\"][\"truncated\"] = 1\n",
    "            detagged_seq[\"seq\"] = detagged_seq[\"seq\"][:job[\"filter_maxlen\"]]\n",
    "            res[\"detagged_seq\"] = detagged_seq\n",
    "            continue\n",
    "\n",
    "        if job[\"filter_maxlen\"] < 0:\n",
    "            res[\"status\"][\"truncated\"] = 1\n",
    "            detagged_seq[\"seq\"] = detagged_seq[\"seq\"][:job[\"filter_maxlen\"]]\n",
    "            res[\"detagged_seq\"] = detagged_seq\n",
    "#            continue\n",
    "\n",
    "#        print res\n",
    "        res[\"keep\"] = True\n",
    "        res[\"id\"] = seq.id\n",
    "        res[\"detag_qual\"] = qual\n",
    "        reproc.process_result(res)\n",
    "\n",
    "    reproc.close_tag_files()\n",
    "    reproc.harvest_stats()\n",
    "    reproc.write_stats()\n",
    "    write_logfile(job[\"output_prefix\"],jobsum, heelsum, tagsum,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on ProcessPool in module pathos.multiprocessing object:\n",
      "\n",
      "class ProcessPool(pathos.abstract_launcher.AbstractWorkerPool)\n",
      " |  Mapper that leverages python's multiprocessing.\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ProcessPool\n",
      " |      pathos.abstract_launcher.AbstractWorkerPool\n",
      " |      __builtin__.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args, **kwds)\n",
      " |      Important class members:\n",
      " |          nodes       - number (and potentially description) of workers\n",
      " |          ncpus       - number of worker processors\n",
      " |          servers     - list of worker servers\n",
      " |          scheduler   - the associated scheduler\n",
      " |          workdir     - associated $WORKDIR for scratch calculations/files\n",
      " |      \n",
      " |      Other class members:\n",
      " |          scatter     - True, if uses 'scatter-gather' (instead of 'worker-pool')\n",
      " |          source      - False, if minimal use of TemporaryFiles is desired\n",
      " |          timeout     - number of seconds to wait for return value from scheduler\n",
      " |              \n",
      " |      NOTE: if number of nodes is not given, will autodetect processors\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      ########################################################################\n",
      " |  \n",
      " |  amap(self, f, *args, **kwds)\n",
      " |      run a batch of jobs with an asynchronous map\n",
      " |      \n",
      " |      Returns a results object which containts the results of applying the\n",
      " |      function f to the items of the argument sequence(s). If more than one\n",
      " |      sequence is given, the function is called with an argument list consisting\n",
      " |      of the corresponding item of each sequence. To retrieve the results, call\n",
      " |      the get() method on the returned results object. The call to get() is\n",
      " |      blocking, until all results are retrieved. Use the ready() method on the\n",
      " |      result object to check if all results are ready.\n",
      " |  \n",
      " |  apipe(self, f, *args, **kwds)\n",
      " |      submit a job asynchronously to a queue\n",
      " |      \n",
      " |      Returns a results object which containts the result of calling the\n",
      " |      function f on a selected worker. To retrieve the results, call the\n",
      " |      get() method on the returned results object. The call to get() is\n",
      " |      blocking, until the result is available. Use the ready() method on the\n",
      " |      results object to check if the result is ready.\n",
      " |  \n",
      " |  clear = _clear(self)\n",
      " |  \n",
      " |  close(self)\n",
      " |      close the pool to any new jobs\n",
      " |  \n",
      " |  imap(self, f, *args, **kwds)\n",
      " |      run a batch of jobs with a non-blocking and ordered map\n",
      " |      \n",
      " |      Returns a list iterator of results of applying the function f to the items\n",
      " |      of the argument sequence(s). If more than one sequence is given, the\n",
      " |      function is called with an argument list consisting of the corresponding\n",
      " |      item of each sequence.\n",
      " |  \n",
      " |  join(self)\n",
      " |      cleanup the closed worker processes\n",
      " |  \n",
      " |  map(self, f, *args, **kwds)\n",
      " |      run a batch of jobs with a blocking and ordered map\n",
      " |      \n",
      " |      Returns a list of results of applying the function f to the items of\n",
      " |      the argument sequence(s). If more than one sequence is given, the\n",
      " |      function is called with an argument list consisting of the corresponding\n",
      " |      item of each sequence.\n",
      " |  \n",
      " |  pipe(self, f, *args, **kwds)\n",
      " |      submit a job and block until results are available\n",
      " |      \n",
      " |      Returns result of calling the function f on a selected worker.  This function\n",
      " |      will block until results are available.\n",
      " |  \n",
      " |  restart(self, force=False)\n",
      " |      restart a closed pool\n",
      " |  \n",
      " |  terminate(self)\n",
      " |      a more abrupt close\n",
      " |  \n",
      " |  uimap(self, f, *args, **kwds)\n",
      " |      run a batch of jobs with a non-blocking and unordered map\n",
      " |      \n",
      " |      Returns a list iterator of results of applying the function f to the items\n",
      " |      of the argument sequence(s). If more than one sequence is given, the\n",
      " |      function is called with an argument list consisting of the corresponding\n",
      " |      item of each sequence. The order of the resulting sequence is not guaranteed.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  ncpus\n",
      " |      get the number of nodes used in the map\n",
      " |  \n",
      " |  nodes\n",
      " |      get the number of nodes used in the map\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from pathos.abstract_launcher.AbstractWorkerPool:\n",
      " |  \n",
      " |  __enter__(self)\n",
      " |  \n",
      " |  __exit__(self, *args)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from pathos.abstract_launcher.AbstractWorkerPool:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pool.__enter__())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
